{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Suwani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful libs\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# STTM lib from Github\n",
    "#from gsdmm import MovieGroupProcess\n",
    "# Custom python scripts for preprocessing, prediction and\n",
    "# visualization that I will define more in depth later\n",
    "#from preprocessing import tokenize\n",
    "#from topic_allocation import top_words, topic_attribution\n",
    "#from visualisation import plot_topic_notebook\n",
    "# Load the 20NewsGroups dataset from sklearn\n",
    "#cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "#newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.read_excel('/Users/Suwani/Desktop/Moodys Project/Cleaned data/apr19_cleaned.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = hdf['Headline'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 991 clusters with 10 clusters populated\n",
      "In stage 1: transferred 734 clusters with 10 clusters populated\n",
      "In stage 2: transferred 513 clusters with 10 clusters populated\n",
      "In stage 3: transferred 401 clusters with 10 clusters populated\n",
      "In stage 4: transferred 357 clusters with 9 clusters populated\n",
      "In stage 5: transferred 350 clusters with 9 clusters populated\n",
      "In stage 6: transferred 349 clusters with 9 clusters populated\n",
      "In stage 7: transferred 331 clusters with 9 clusters populated\n",
      "In stage 8: transferred 342 clusters with 9 clusters populated\n",
      "In stage 9: transferred 316 clusters with 9 clusters populated\n",
      "In stage 10: transferred 255 clusters with 8 clusters populated\n",
      "In stage 11: transferred 271 clusters with 8 clusters populated\n",
      "In stage 12: transferred 260 clusters with 8 clusters populated\n",
      "In stage 13: transferred 210 clusters with 7 clusters populated\n",
      "In stage 14: transferred 202 clusters with 7 clusters populated\n",
      "In stage 15: transferred 208 clusters with 7 clusters populated\n",
      "In stage 16: transferred 216 clusters with 7 clusters populated\n",
      "In stage 17: transferred 218 clusters with 7 clusters populated\n",
      "In stage 18: transferred 221 clusters with 6 clusters populated\n",
      "In stage 19: transferred 226 clusters with 6 clusters populated\n",
      "In stage 20: transferred 213 clusters with 6 clusters populated\n",
      "In stage 21: transferred 204 clusters with 6 clusters populated\n",
      "In stage 22: transferred 214 clusters with 6 clusters populated\n",
      "In stage 23: transferred 215 clusters with 6 clusters populated\n",
      "In stage 24: transferred 202 clusters with 6 clusters populated\n",
      "In stage 25: transferred 197 clusters with 6 clusters populated\n",
      "In stage 26: transferred 218 clusters with 6 clusters populated\n",
      "In stage 27: transferred 204 clusters with 6 clusters populated\n",
      "In stage 28: transferred 201 clusters with 6 clusters populated\n",
      "In stage 29: transferred 189 clusters with 6 clusters populated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Save model\\nwith open(\\'dumps/trained_models/model_v2.model\\', \"wb\") as f:\\n    pickle.dump(mgp, f)\\n    f.close()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new model \n",
    "\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "mgp = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "n_docs = len(docs)\n",
    "\n",
    "# Fit the model on the data given the chosen seeds\n",
    "y = mgp.fit(docs, n_terms)\n",
    "\n",
    "'''\n",
    "# Save model\n",
    "with open('dumps/trained_models/model_v2.model', \"wb\") as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topics : [  0 199   3   4   0   0 441  34   0 491]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [9 6 1 7 3 2 8 5 4 0]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "from numpy import log, exp\n",
    "from numpy import argmax\n",
    "import json\n",
    "\n",
    "class MovieGroupProcess:\n",
    "    def __init__(self, K=8, alpha=0.1, beta=0.1, n_iters=30):\n",
    "        '''\n",
    "        A MovieGroupProcess is a conceptual model introduced by Yin and Wang 2014 to\n",
    "        describe their Gibbs sampling algorithm for a Dirichlet Mixture Model for the\n",
    "        clustering short text documents.\n",
    "        Reference: http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        Imagine a professor is leading a film class. At the start of the class, the students\n",
    "        are randomly assigned to K tables. Before class begins, the students make lists of\n",
    "        their favorite films. The teacher reads the role n_iters times. When\n",
    "        a student is called, the student must select a new table satisfying either:\n",
    "            1) The new table has more students than the current table.\n",
    "        OR\n",
    "            2) The new table has students with similar lists of favorite movies.\n",
    "        :param K: int\n",
    "            Upper bound on the number of possible clusters. Typically many fewer\n",
    "        :param alpha: float between 0 and 1\n",
    "            Alpha controls the probability that a student will join a table that is currently empty\n",
    "            When alpha is 0, no one will join an empty table.\n",
    "        :param beta: float between 0 and 1\n",
    "            Beta controls the student's affinity for other students with similar interests. A low beta means\n",
    "            that students desire to sit with students of similar interests. A high beta means they are less\n",
    "            concerned with affinity and are more influenced by the popularity of a table\n",
    "        :param n_iters:\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "        # slots for computed variables\n",
    "        self.number_docs = None\n",
    "        self.vocab_size = None\n",
    "        self.cluster_doc_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_distribution = [{} for i in range(K)]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_data(K, alpha, beta, D, vocab_size, cluster_doc_count, cluster_word_count, cluster_word_distribution):\n",
    "        '''\n",
    "        Reconstitute a MovieGroupProcess from previously fit data\n",
    "        :param K:\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param D:\n",
    "        :param vocab_size:\n",
    "        :param cluster_doc_count:\n",
    "        :param cluster_word_count:\n",
    "        :param cluster_word_distribution:\n",
    "        :return:\n",
    "        '''\n",
    "        mgp = MovieGroupProcess(K, alpha, beta, n_iters=30)\n",
    "        mgp.number_docs = D\n",
    "        mgp.vocab_size = vocab_size\n",
    "        mgp.cluster_doc_count = cluster_doc_count\n",
    "        mgp.cluster_word_count = cluster_word_count\n",
    "        mgp.cluster_word_distribution = cluster_word_distribution\n",
    "        return mgp\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(p):\n",
    "        '''\n",
    "        Sample with probability vector p from a multinomial distribution\n",
    "        :param p: list\n",
    "            List of probabilities representing probability vector for the multinomial distribution\n",
    "        :return: int\n",
    "            index of randomly selected output\n",
    "        '''\n",
    "        return [i for i, entry in enumerate(multinomial(1, p)) if entry != 0][0]\n",
    "\n",
    "    def fit(self, docs, vocab_size):\n",
    "        '''\n",
    "        Cluster the input documents\n",
    "        :param docs: list of list\n",
    "            list of lists containing the unique token set of each document\n",
    "        :param V: total vocabulary size for each document\n",
    "        :return: list of length len(doc)\n",
    "            cluster label for each document\n",
    "        '''\n",
    "        alpha, beta, K, n_iters, V = self.alpha, self.beta, self.K, self.n_iters, vocab_size\n",
    "\n",
    "        D = len(docs)\n",
    "        self.number_docs = D\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # unpack to easy var names\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "        cluster_count = K\n",
    "        d_z = [None for i in range(len(docs))]\n",
    "\n",
    "        # initialize the clusters\n",
    "        for i, doc in enumerate(docs):\n",
    "\n",
    "            # choose a random  initial cluster for the doc\n",
    "            z = self._sample([1.0 / K for _ in range(K)])\n",
    "            d_z[i] = z\n",
    "            m_z[z] += 1\n",
    "            n_z[z] += len(doc)\n",
    "\n",
    "            for word in doc:\n",
    "                if word not in n_z_w[z]:\n",
    "                    n_z_w[z][word] = 0\n",
    "                n_z_w[z][word] += 1\n",
    "\n",
    "        for _iter in range(n_iters):\n",
    "            total_transfers = 0\n",
    "\n",
    "            for i, doc in enumerate(docs):\n",
    "\n",
    "                # remove the doc from it's current cluster\n",
    "                z_old = d_z[i]\n",
    "\n",
    "                m_z[z_old] -= 1\n",
    "                n_z[z_old] -= len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    n_z_w[z_old][word] -= 1\n",
    "\n",
    "                    # compact dictionary to save space\n",
    "                    if n_z_w[z_old][word] == 0:\n",
    "                        del n_z_w[z_old][word]\n",
    "\n",
    "                # draw sample from distribution to find new cluster\n",
    "                p = self.score(doc)\n",
    "                z_new = self._sample(p)\n",
    "\n",
    "                # transfer doc to the new cluster\n",
    "                if z_new != z_old:\n",
    "                    total_transfers += 1\n",
    "\n",
    "                d_z[i] = z_new\n",
    "                m_z[z_new] += 1\n",
    "                n_z[z_new] += len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    if word not in n_z_w[z_new]:\n",
    "                        n_z_w[z_new][word] = 0\n",
    "                    n_z_w[z_new][word] += 1\n",
    "\n",
    "            cluster_count_new = sum([1 for v in m_z if v > 0])\n",
    "            print(\"In stage %d: transferred %d clusters with %d clusters populated\" % (\n",
    "            _iter, total_transfers, cluster_count_new))\n",
    "            if total_transfers == 0 and cluster_count_new == cluster_count and _iter>25:\n",
    "                print(\"Converged.  Breaking out.\")\n",
    "                break\n",
    "            cluster_count = cluster_count_new\n",
    "        self.cluster_word_distribution = n_z_w\n",
    "        return d_z\n",
    "\n",
    "    def score(self, doc):\n",
    "        '''\n",
    "        Score a document\n",
    "        Implements formula (3) of Yin and Wang 2014.\n",
    "        http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return: list[float]: A length K probability vector where each component represents\n",
    "                              the probability of the document appearing in a particular cluster\n",
    "        '''\n",
    "        alpha, beta, K, V, D = self.alpha, self.beta, self.K, self.vocab_size, self.number_docs\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "\n",
    "        p = [0 for _ in range(K)]\n",
    "\n",
    "        #  We break the formula into the following pieces\n",
    "        #  p = N1*N2/(D1*D2) = exp(lN1 - lD1 + lN2 - lD2)\n",
    "        #  lN1 = log(m_z[z] + alpha)\n",
    "        #  lN2 = log(D - 1 + K*alpha)\n",
    "        #  lN2 = log(product(n_z_w[w] + beta)) = sum(log(n_z_w[w] + beta))\n",
    "        #  lD2 = log(product(n_z[d] + V*beta + i -1)) = sum(log(n_z[d] + V*beta + i -1))\n",
    "\n",
    "        lD1 = log(D - 1 + K * alpha)\n",
    "        doc_size = len(doc)\n",
    "        for label in range(K):\n",
    "            lN1 = log(m_z[label] + alpha)\n",
    "            lN2 = 0\n",
    "            lD2 = 0\n",
    "            for word in doc:\n",
    "                lN2 += log(n_z_w[label].get(word, 0) + beta)\n",
    "            for j in range(1, doc_size +1):\n",
    "                lD2 += log(n_z[label] + V * beta + j - 1)\n",
    "            p[label] = exp(lN1 - lD1 + lN2 - lD2)\n",
    "\n",
    "        # normalize the probability vector\n",
    "        pnorm = sum(p)\n",
    "        pnorm = pnorm if pnorm>0 else 1\n",
    "        return [pp/pnorm for pp in p]\n",
    "\n",
    "    def choose_best_label(self, doc):\n",
    "        '''\n",
    "        Choose the highest probability label for the input document\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return:\n",
    "        '''\n",
    "        p = self.score(doc)\n",
    "        return argmax(p),max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libs\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# STTM lib from Github\n",
    "#from gsdmm import MovieGroupProcess\n",
    "# Custom python scripts for preprocessing, prediction and\n",
    "# visualization that I will define more in depth later\n",
    "#from preprocessing import tokenize\n",
    "#from topic_allocation import top_words, topic_attribution\n",
    "#from visualisation import plot_topic_notebook\n",
    "# Load the 20NewsGroups dataset from sklearn\n",
    "cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
